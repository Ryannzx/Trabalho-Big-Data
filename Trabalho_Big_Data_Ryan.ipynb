{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JX1u0CuvTW1XdyKbSh2oDB6AtG3L1YHg",
      "authorship_tag": "ABX9TyOSCkcZGONuQ7HNhiDj6/ZD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importação Arquivo para o Google Colab"
      ],
      "metadata": {
        "id": "0-veDEvDGdf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFI9y3gWGZBc"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/Engenharia-De-Software/Big-Data/imdb-reviews-pt-br.csv /content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacao pyspark"
      ],
      "metadata": {
        "id": "0rYeNMTPRiwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pihJbbp4RrNr",
        "outputId": "76da58c1-195e-40b5-88c0-74e820b539ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e1179d5459412c768bec25fb779ae787e4210699900b4867f0e2de74d19be7b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar a biblioteca e criar uma seção Spark"
      ],
      "metadata": {
        "id": "5FqHSR3nSVQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "PbTa9EbBSYTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar DataFrame com o Spark"
      ],
      "metadata": {
        "id": "b-5NSpfFd4-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "m60lA87Md_Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostrar o resultado"
      ],
      "metadata": {
        "id": "sNADGOgT_446"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwczwjXYeXmN",
        "outputId": "290b2316-ef5c-4300-c4c8-864b9487b93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+---------+\n",
            "| id|             text_en|             text_pt|sentiment|\n",
            "+---+--------------------+--------------------+---------+\n",
            "|  1|Once again Mr. Co...|Mais uma vez, o S...|      neg|\n",
            "|  2|This is an exampl...|Este é um exemplo...|      neg|\n",
            "|  3|First of all I ha...|Primeiro de tudo ...|      neg|\n",
            "|  4|Not even the Beat...|Nem mesmo os Beat...|      neg|\n",
            "|  5|Brass pictures mo...|Filmes de fotos d...|      neg|\n",
            "|  6|A funny thing hap...|Uma coisa engraça...|      neg|\n",
            "|  7|This German horro...|Este filme de ter...|      neg|\n",
            "|  8|Being a long-time...|Sendo um fã de lo...|      neg|\n",
            "|  9|\"Tokyo Eyes\" tell...|\"Tokyo Eyes\" fala...|      neg|\n",
            "| 10|Wealthy horse ran...|Fazendeiros ricos...|      neg|\n",
            "| 11|Cage plays a drun...|Cage interpreta u...|      neg|\n",
            "| 12|First of all, I w...|Primeiro de tudo,...|      neg|\n",
            "| 13|So tell me - what...|Então me diga - q...|      neg|\n",
            "| 14|A big disappointm...|Uma grande decepç...|      neg|\n",
            "| 15|This film is abso...|Este filme é abso...|      neg|\n",
            "| 16|Heres a decidedly...|Heres um decidida...|      neg|\n",
            "| 17|At the bottom end...|Na parte inferior...|      neg|\n",
            "| 18|Earth has been de...|A terra foi destr...|      neg|\n",
            "| 19|Many people are s...|Muitas pessoas es...|      neg|\n",
            "| 20|New York family i...|A família de Nova...|      neg|\n",
            "+---+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devo criar um código utilizando funções MAP e REDUCE para exibir a soma dos ID's de comentários classificados como 'Neg' ou Negativo"
      ],
      "metadata": {
        "id": "vXfHkDUy9njW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação da função MAP para mapear o sententiment como chave e o id como valor inteiro\n",
        "def funcao_map(row):\n",
        "    sentimento = row['sentiment']\n",
        "    valor_id = int(row['id'])\n",
        "    return (sentimento, valor_id)\n",
        "\n",
        "# Criação da função REDUCE para somar os IDs\n",
        "def funcao_reduce(x, y):\n",
        "    return x + y\n",
        "\n",
        "# Aplicação das funções MAP e REDUCE\n",
        "rdd_mapeado = imdb_df.rdd.map(funcao_map)\n",
        "rdd_reduzido = rdd_mapeado.reduceByKey(funcao_reduce)\n",
        "\n",
        "# Coletar resultados\n",
        "resultado = rdd_reduzido.collect()\n",
        "\n",
        "# Exibir a soma dos IDs classificados como 'negativo'\n",
        "for sentimento, id_total in resultado:\n",
        "    if sentimento == 'neg':\n",
        "        print(\"Soma dos IDs classificados como 'negativo':\", id_total)"
      ],
      "metadata": {
        "id": "JQtsTEVZ7g4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Já aqui devo criar um código com funções MAP e REDUCE para exibir a diferença entre a soma das palavras dos textos em português e inglês classificados como \"neg\""
      ],
      "metadata": {
        "id": "NMLvs36BByUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Criando a sessão Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Função MAP para mapear o sentimento como chave e uma tupla com a soma das palavras de cada texto\n",
        "def contagem_palavras_map(row):\n",
        "    sentimento = row['sentiment']\n",
        "    texto_ingles = len(row['text_en'].split())\n",
        "    texto_portugues = len(row['text_pt'].split())\n",
        "    return (sentimento, (texto_ingles, texto_portugues))\n",
        "\n",
        "# Função REDUCE para somar o número total de palavras dos textos em inglês e português por sentimento\n",
        "def soma_palavras_red(x, y):\n",
        "    return (x[0] + y[0], x[1] + y[1])\n",
        "\n",
        "# Aplicando o MAP e REDUCE no DataFrame\n",
        "rdd_map_red = imdb_df.rdd.map(contagem_palavras_map).reduceByKey(soma_palavras_red)\n",
        "\n",
        "# Coletando os resultados\n",
        "resultados = rdd_map_red.collect()\n",
        "\n",
        "# Selecionando os dados relacionados aos textos negativos\n",
        "contagem_neg = dict(resultados)['neg']\n",
        "\n",
        "# Calculando a diferença entre as somas de palavras dos textos em português e inglês classificados como negativos\n",
        "diferenca = contagem_neg[1] - contagem_neg[0]\n",
        "\n",
        "print(\"Diferença entre a soma das palavras em português e inglês dos textos classificados como 'neg'é\", diferenca,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIV1pB1KBx1Q",
        "outputId": "bc57fb92-265e-4a2a-aa76-0dd977b2a097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diferença entre a soma das palavras em português e inglês dos textos classificados como 'neg'é 54949\n"
          ]
        }
      ]
    }
  ]
}